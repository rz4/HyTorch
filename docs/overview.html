

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>overview &mdash; MinoTauro 0.0.9 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="tutorials" href="tutorials/index.html" />
    <link rel="prev" title="MinoTauro (0.1.0) API" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html">
          

          
            
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#motivation">Motivation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#features">Features</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pytorch-computational-graphs-as-s-expressions">PyTorch Computational Graphs as S-Expressions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#anonymous-mu-expressions-i-e-anonymous-pytorch-modules">Anonymous Mu Expressions (i.e. Anonymous PyTorch Modules)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#reverting-models-to-s-expressions">Reverting Models to S-Expressions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#advanced-expression-threading">Advanced Expression Threading</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spec-clojure-like-specifications-for-pytorch">Spec (Clojure-like Specifications For PyTorch)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#getting-started">Getting Started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#jupyter-notebook-running-minotauro">Jupyter Notebook Running MinoTauro</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tutorials-api">Tutorials &amp; API</a><ul class="simple">
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/index.html">tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="mino/index.html">mino</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MinoTauro</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>overview</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/overview.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="overview">
<h1>overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h2>
<p>The dynamic execution of PyTorch operations allows enough flexibility to change
computational graphs on the fly. This provides an avenue for Hy, a lisp-binding
library for Python, to be used in establishing meta-programming practices in the
field of differential learning (DL).</p>
<p>While the final goal of this project is to build a framework which will allow
DL systems to have access to their code during runtime, this coding paradigm
also shows promise at accelerating the development of new differential models
while promoting formalized abstraction with predicate type checking. A common
trend in current DL packages is an abundance of opaque object-oriented abstraction
with packages such as Keras. This only reduces transparency to the already
black-box nature of neural network (NN) systems, and makes interpretability and
reproducibility of models more difficult.</p>
<p>In order to better understand DL models and allow for quick iterative design
over novel or esoteric architectures, programmers require access to an
environment which allows low-level definition of computational graphs and
provides methods to quickly access network components for refactoring, debugging and analysis,
while still providing gpu-acceleration. The added expressibility of Lisp in
combination with PyTorch’s functional API allows for this type of programming
paradigm, and provides DL researchers an extendable framework which is not
matched by abstractions allowed in contemporary NN packages.</p>
</div>
<div class="section" id="features">
<h2>Features<a class="headerlink" href="#features" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pytorch-computational-graphs-as-s-expressions">
<h3>PyTorch Computational Graphs as S-Expressions<a class="headerlink" href="#pytorch-computational-graphs-as-s-expressions" title="Permalink to this headline">¶</a></h3>
<p>Defining models using S-expressions allows for functional design, quick iterative
refactoring, and manipulation of model code using macros. Here is a short example
of defining a single layer feed forward neural network using MinoTauro and
then training a generated model on dummy data:</p>
<div class="highlight-clojure notranslate"><div class="highlight"><pre><span></span><span class="c1">;- Macros</span>
<span class="p">(</span><span class="nf">require</span> <span class="p">[</span><span class="nv">mino.mu</span> <span class="p">[</span><span class="nv">*</span><span class="p">]]</span>
         <span class="p">[</span><span class="nv">mino.thread</span> <span class="p">[</span><span class="nv">*</span><span class="p">]]</span>
         <span class="p">[</span><span class="nv">hy.contrib.walk</span> <span class="p">[</span><span class="nv">let</span><span class="p">]])</span>

<span class="c1">;- Imports</span>
<span class="p">(</span><span class="nb">import </span><span class="nv">torch</span>
        <span class="p">[</span><span class="nv">torch.nn.functional</span> <span class="ss">:as</span> <span class="nv">F</span><span class="p">]</span>
        <span class="p">[</span><span class="nv">torch.nn</span> <span class="ss">:as</span> <span class="nv">nn</span><span class="p">]</span>
        <span class="p">[</span><span class="nv">torch.optim</span> <span class="p">[</span><span class="nv">Adam</span><span class="p">]])</span>

<span class="c1">;-- Defines a Linear Transformation operation</span>
<span class="p">(</span><span class="nf">defmu</span> <span class="nv">LinearTransformation</span> <span class="p">[</span><span class="nv">x</span> <span class="nv">weights</span> <span class="nv">bias</span><span class="p">]</span>
  <span class="p">(</span><span class="nb">-&gt; </span><span class="nv">x</span> <span class="p">(</span><span class="o">@</span> <span class="nv">weights</span><span class="p">)</span> <span class="p">(</span><span class="nb">+ </span><span class="nv">bias</span><span class="p">)))</span>

<span class="c1">;-- Defines a constructor for a Linear Transformation with learnable parameters</span>
<span class="p">(</span><span class="kd">defn </span><span class="nv">LearnableLinear</span> <span class="p">[</span><span class="nv">f-in</span> <span class="nv">f-out</span><span class="p">]</span>
  <span class="p">(</span><span class="nf">LinearTransformation</span>
    <span class="ss">:weights</span> <span class="p">(</span><span class="nb">-&gt; </span><span class="p">(</span><span class="nf">torch.empty</span> <span class="p">(</span>, <span class="nv">f-in</span> <span class="nv">f-out</span><span class="p">))</span>
                 <span class="p">(</span><span class="nf">.normal_</span> <span class="ss">:mean</span> <span class="mi">0</span> <span class="ss">:std</span> <span class="mf">1.0</span><span class="p">)</span>
                 <span class="p">(</span><span class="nf">nn.Parameter</span> <span class="ss">:requires_grad</span> <span class="nv">True</span><span class="p">))</span>
    <span class="ss">:bias</span>    <span class="p">(</span><span class="nb">-&gt; </span><span class="p">(</span><span class="nf">torch.empty</span> <span class="p">(</span>, <span class="nv">f-out</span><span class="p">))</span>
                 <span class="p">(</span><span class="nf">.normal_</span> <span class="ss">:mean</span> <span class="mi">0</span> <span class="ss">:std</span> <span class="mf">1.0</span><span class="p">)</span>
                 <span class="p">(</span><span class="nf">nn.Parameter</span> <span class="ss">:requires_grad</span> <span class="nv">True</span><span class="p">))))</span>

<span class="c1">;-- Defines a Feed Forward operation</span>
<span class="p">(</span><span class="nf">defmu</span> <span class="nv">FeedForward</span> <span class="p">[</span><span class="nv">x</span> <span class="nv">linear-to-hidden</span> <span class="nv">linear-to-output</span><span class="p">]</span>
  <span class="p">(</span><span class="nb">-&gt; </span><span class="nv">x</span>
      <span class="nv">linear-to-hidden</span>
      <span class="nv">torch.sigmoid</span>
      <span class="nv">linear-to-output</span><span class="p">))</span>

<span class="c1">;-- Defines a constructor for a single-layer Neural Network with learnable mappings</span>
<span class="p">(</span><span class="kd">defn </span><span class="nv">NeuralNetwork</span> <span class="p">[</span><span class="nv">nb-inputs</span> <span class="nv">nb-hidden</span> <span class="nv">nb-outputs</span><span class="p">]</span>
  <span class="p">(</span><span class="nf">FeedForward</span>
    <span class="ss">:linear-to-hidden</span> <span class="p">(</span><span class="nf">LearnableLinear</span> <span class="nv">nb-inputs</span> <span class="nv">nb-hidden</span><span class="p">)</span>
    <span class="ss">:linear-to-output</span> <span class="p">(</span><span class="nf">LearnableLinear</span> <span class="nv">nb-hidden</span> <span class="nv">nb-outputs</span><span class="p">)))</span>

<span class="c1">;--</span>
<span class="p">(</span><span class="nf">defmain</span> <span class="p">[</span><span class="o">&amp;</span><span class="nb">rest </span><span class="nv">_</span><span class="p">]</span>

  <span class="p">(</span><span class="nb">print </span><span class="s">&quot;Loading Model + Data...&quot;</span><span class="p">)</span>
  <span class="p">(</span><span class="k">let </span><span class="p">[</span><span class="nv">nb-inputs</span> <span class="mi">10</span> <span class="nv">nb-hidden</span> <span class="mi">32</span> <span class="nv">nb-outputs</span> <span class="mi">1</span><span class="p">]</span>

    <span class="c1">;- Defines Model + Optimizer</span>
    <span class="p">(</span><span class="nf">setv</span> <span class="nv">model</span> <span class="p">(</span><span class="nf">NeuralNetwork</span> <span class="nv">nb-inputs</span> <span class="nv">nb-hidden</span> <span class="nv">nb-outputs</span><span class="p">)</span>
          <span class="nv">optimizer</span> <span class="p">(</span><span class="nf">Adam</span> <span class="p">(</span><span class="nf">.parameters</span> <span class="nv">model</span><span class="p">)</span> <span class="ss">:lr</span> <span class="mf">0.001</span> <span class="ss">:weight_decay</span> <span class="mi">1</span><span class="nv">e-5</span><span class="p">))</span>

    <span class="c1">;- Generates Dummy Data</span>
    <span class="p">(</span><span class="k">let </span><span class="p">[</span><span class="nv">batch-size</span> <span class="mi">100</span><span class="p">]</span>
      <span class="p">(</span><span class="nf">setv</span> <span class="nv">x</span> <span class="p">(</span><span class="nb">-&gt; </span><span class="p">(</span><span class="nf">torch.empty</span> <span class="p">(</span>, <span class="nv">batch-size</span> <span class="nv">nb-inputs</span><span class="p">))</span>
                  <span class="p">(</span><span class="nf">.normal_</span> <span class="ss">:mean</span> <span class="mi">0</span> <span class="ss">:std</span> <span class="mf">1.0</span><span class="p">))</span>
            <span class="nv">y</span> <span class="p">(</span><span class="nf">torch.ones</span> <span class="p">(</span>, <span class="nv">batch-size</span> <span class="nv">nb-outputs</span><span class="p">))))</span>

  <span class="c1">;- Train</span>
  <span class="p">(</span><span class="k">let </span><span class="p">[</span><span class="nv">epochs</span> <span class="mi">100</span><span class="p">]</span>
    <span class="p">(</span><span class="nb">print </span><span class="s">&quot;Training...&quot;</span><span class="p">)</span>
    <span class="p">(</span><span class="nb">for </span><span class="p">[</span><span class="nv">epoch</span> <span class="p">(</span><span class="nb">range </span><span class="nv">epochs</span><span class="p">)]</span>

      <span class="c1">;- Forward</span>
      <span class="p">(</span><span class="nf">setv</span> <span class="nv">y-pred</span> <span class="p">(</span><span class="nf">model</span> <span class="nv">x</span><span class="p">))</span>
      <span class="p">(</span><span class="nf">setv</span> <span class="nv">loss</span> <span class="p">(</span><span class="nf">F.binary_cross_entropy_with_logits</span> <span class="nv">y-pred</span> <span class="nv">y</span><span class="p">))</span>
      <span class="p">(</span><span class="nb">print </span><span class="p">(</span><span class="nf">.format</span> <span class="s">&quot;Epoch: {epoch} Loss: {loss}&quot;</span> <span class="ss">:epoch</span> <span class="nv">epoch</span> <span class="ss">:loss</span> <span class="nv">loss</span><span class="p">))</span>

      <span class="c1">;- Backward</span>
      <span class="p">(</span><span class="nf">.zero_grad</span> <span class="nv">optimizer</span><span class="p">)</span>
      <span class="p">(</span><span class="nf">.backward</span> <span class="nv">loss</span><span class="p">)</span>
      <span class="p">(</span><span class="nf">.step</span> <span class="nv">optimizer</span><span class="p">))))</span>
</pre></div>
</div>
<p>PyTorch auto-differential system
works through definitions of models as <code class="docutils literal notranslate"><span class="pre">Modules</span></code> which are used to organize
operations and dependent learnable parameters.
MinoTauro extends PyTorch’s abstractions by letting you define computational graphs
in functional-syntax through Minotauro’s <code class="docutils literal notranslate"><span class="pre">mu</span></code> expressions.
In short, Minotauro makes writing new modules as simple as writing a new lambda expression.</p>
<p>In the above example, we show the use of the macro <code class="docutils literal notranslate"><span class="pre">defmu</span></code> which takes its arguments and
defines a PyTorch <code class="docutils literal notranslate"><span class="pre">Module</span></code> class. The <code class="docutils literal notranslate"><span class="pre">components</span></code> used by the module during forward
propagation are defined in the argument list. The expressions following the argument list
defines the <code class="docutils literal notranslate"><span class="pre">forward-procedure</span></code>. <code class="docutils literal notranslate"><span class="pre">components</span></code> are typically <code class="docutils literal notranslate"><span class="pre">torch.nn.Parameter</span></code> or
<code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> .</p>
<p>Thus, defining a PyTorch module takes on the following form:</p>
<div class="highlight-clojure notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="nf">defmu</span> <span class="nv">module-name</span> <span class="p">[</span><span class="nv">component-0</span> <span class="nv">...</span> <span class="nv">component-N</span><span class="p">]</span> <span class="nv">forward-procedure</span><span class="p">)</span>
</pre></div>
</div>
<p>While PyTorch’s module system uses an object oriented approach, MinoTauro’s abstractions allows
for functional manipulation of tensor objects. MinoTauro abstracts the PyTorch <code class="docutils literal notranslate"><span class="pre">Module</span></code> into
the form <code class="docutils literal notranslate"><span class="pre">mu</span></code>. A <code class="docutils literal notranslate"><span class="pre">mu</span></code> can be thought of as a lambda expression with all the added
benefits of PyTorch’s <code class="docutils literal notranslate"><span class="pre">Module</span></code> system. This means all native PyTorch operations
still work including moving PyTorch objects to and from devices and accessing sub-modules
and parameters.</p>
<p>Default <code class="docutils literal notranslate"><span class="pre">components</span></code> (or sub-modules in traditional PyTorch) can be binded to
<code class="docutils literal notranslate"><span class="pre">mu</span></code> when creating a new object. If bound during initialization, the default <code class="docutils literal notranslate"><span class="pre">components</span></code>
will be used during the forward pass if not provided in the function call.</p>
<p>As an example, the <code class="docutils literal notranslate"><span class="pre">LearnableLinear</span></code> function in the above script generates a new
<code class="docutils literal notranslate"><span class="pre">LinearTransformation</span></code> module with custom default-and-persistent tensors, <code class="docutils literal notranslate"><span class="pre">weights</span></code>
and <code class="docutils literal notranslate"><span class="pre">bias</span></code>. If arguments <code class="docutils literal notranslate"><span class="pre">weights</span></code> or <code class="docutils literal notranslate"><span class="pre">bias</span></code> are not provided
during the forward pass of <code class="docutils literal notranslate"><span class="pre">LinearTransformation</span></code>, then these default values are used instead.</p>
<p>We can view a representation of the computational graph by printing the model:</p>
<div class="highlight-clojure notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="nb">print </span><span class="nv">model</span><span class="p">)</span>

<span class="c1">;- Returns</span>
<span class="s">&quot;&quot;&quot;</span>
<span class="s">FeedForward(</span>
<span class="s">  At: 0x10c5e3850</span>
<span class="s">  C: [x linear-to-hidden linear-to-output]</span>
<span class="s">  λ: (linear-to-output (torch.sigmoid (linear-to-hidden x)))</span>


<span class="s">  (linear_to_hidden): LinearTransformation(</span>
<span class="s">    At: 0x10c7b22d0</span>
<span class="s">    C: [x weights bias]</span>
<span class="s">    λ: (+ (@ x weights) bias)</span>

<span class="s">    weights: (Parameter :size [10 10] :dtype torch.float32)</span>
<span class="s">    bias: (Parameter :size [10] :dtype torch.float32)</span>

<span class="s">  )</span>
<span class="s">  (linear_to_output): LinearTransformation(</span>
<span class="s">    At: 0x1297e71d0</span>
<span class="s">    C: [x weights bias]</span>
<span class="s">    λ: (+ (@ x weights) bias)</span>

<span class="s">    weights: (Parameter :size [10 1] :dtype torch.float32)</span>
<span class="s">    bias: (Parameter :size [1] :dtype torch.float32)</span>

<span class="s">  )</span>
<span class="s">)</span>
<span class="s">&quot;&quot;&quot;</span>
</pre></div>
</div>
<p>Accessing and viewing any component of the model is simple and done through python’s dot notation.
This make exploring a model easy and intuitive:</p>
<div class="highlight-clojure notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="nb">print </span><span class="nv">model.linear-to-hidden</span><span class="p">)</span>

<span class="c1">;- Returns</span>
<span class="s">&quot;&quot;&quot;</span>
<span class="s">LinearTransformation(</span>
<span class="s">  At: 0x10c7b22d0</span>
<span class="s">  C: [x weights bias]</span>
<span class="s">  λ: (+ (@ x weights) bias)</span>

<span class="s">  weights: (Parameter :size [10 10] :dtype torch.float32)</span>
<span class="s">  bias: (Parameter :size [10] :dtype torch.float32)</span>

<span class="s">)</span>
<span class="s">&quot;&quot;&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="anonymous-mu-expressions-i-e-anonymous-pytorch-modules">
<h3>Anonymous Mu Expressions (i.e. Anonymous PyTorch Modules)<a class="headerlink" href="#anonymous-mu-expressions-i-e-anonymous-pytorch-modules" title="Permalink to this headline">¶</a></h3>
<p>Side effects make systems more difficult to debug and understand. The <code class="docutils literal notranslate"><span class="pre">mu</span></code> was designed to
reduce the <code class="docutils literal notranslate"><span class="pre">Module</span></code> to an abstraction similar to lambda expressions. MinoTauro allows
for anonymous PyTorch <code class="docutils literal notranslate"><span class="pre">Modules</span></code> through <code class="docutils literal notranslate"><span class="pre">mu</span></code>. For example, an anonymous Linear function
can be defined as follows:</p>
<div class="highlight-clojure notranslate"><div class="highlight"><pre><span></span><span class="c1">;- Anonymous Linear</span>
<span class="p">(</span><span class="nf">mu</span> <span class="p">[</span><span class="nv">x</span> <span class="nv">w</span> <span class="nv">b</span><span class="p">]</span> <span class="p">(</span><span class="nb">-&gt; </span><span class="nv">x</span> <span class="p">(</span><span class="o">@</span> <span class="nv">w</span><span class="p">)</span> <span class="p">(</span><span class="nb">+ </span><span class="nv">b</span><span class="p">)))</span>

<span class="c1">;- Forward Propagate</span>
<span class="p">((</span><span class="nf">mu</span> <span class="p">[</span><span class="nv">x</span> <span class="nv">w</span> <span class="nv">b</span><span class="p">]</span> <span class="p">(</span><span class="nb">-&gt; </span><span class="nv">x</span> <span class="p">(</span><span class="o">@</span> <span class="nv">w</span><span class="p">)</span> <span class="p">(</span><span class="nb">+ </span><span class="nv">b</span><span class="p">)))</span> <span class="nv">my-x</span> <span class="nv">my-w</span> <span class="nv">my-b</span><span class="p">)</span>
</pre></div>
</div>
<p>MinoTauro’s macro <code class="docutils literal notranslate"><span class="pre">bind</span></code> can be used to assign default values to components same as when creating
a new object of a namespaced <code class="docutils literal notranslate"><span class="pre">mu</span></code> with <code class="docutils literal notranslate"><span class="pre">defmu</span></code>. Using the Linear function as an example again:</p>
<div class="highlight-clojure notranslate"><div class="highlight"><pre><span></span><span class="c1">;- Anonymous Linear with default w and b</span>
<span class="p">(</span><span class="nf">bind</span> <span class="p">(</span><span class="nf">mu</span> <span class="p">[</span><span class="nv">x</span> <span class="nv">w</span> <span class="nv">b</span><span class="p">]</span> <span class="p">(</span><span class="nb">-&gt; </span><span class="nv">x</span> <span class="p">(</span><span class="o">@</span> <span class="nv">w</span><span class="p">)</span> <span class="p">(</span><span class="nb">+ </span><span class="nv">b</span><span class="p">)))</span>
  <span class="ss">:w</span> <span class="p">(</span><span class="nb">-&gt; </span><span class="p">(</span><span class="nf">torch.empty</span> <span class="p">(</span>, <span class="nv">f-in</span> <span class="nv">f-out</span><span class="p">))</span>
         <span class="p">(</span><span class="nf">.normal_</span> <span class="ss">:mean</span> <span class="mi">0</span> <span class="ss">:std</span> <span class="mf">1.0</span><span class="p">)</span>
         <span class="p">(</span><span class="nf">Parameter</span> <span class="ss">:requires_grad</span> <span class="nv">True</span><span class="p">))</span>
  <span class="ss">:b</span> <span class="p">(</span><span class="nb">-&gt; </span><span class="p">(</span><span class="nf">torch.empty</span> <span class="p">(</span>, <span class="nv">f-out</span><span class="p">))</span>
         <span class="p">(</span><span class="nf">.normal_</span> <span class="ss">:mean</span> <span class="mi">0</span> <span class="ss">:std</span> <span class="mf">1.0</span><span class="p">)</span>
         <span class="p">(</span><span class="nf">Parameter</span> <span class="ss">:requires_grad</span> <span class="nv">True</span><span class="p">))))</span>

<span class="c1">;- Namespaced Linear with default w and b</span>
<span class="p">(</span><span class="nf">Linear</span>
  <span class="ss">:w</span> <span class="p">(</span><span class="nb">-&gt; </span><span class="p">(</span><span class="nf">torch.empty</span> <span class="p">(</span>, <span class="nv">f-in</span> <span class="nv">f-out</span><span class="p">))</span>
         <span class="p">(</span><span class="nf">.normal_</span> <span class="ss">:mean</span> <span class="mi">0</span> <span class="ss">:std</span> <span class="mf">1.0</span><span class="p">)</span>
         <span class="p">(</span><span class="nf">Parameter</span> <span class="ss">:requires_grad</span> <span class="nv">True</span><span class="p">))</span>
  <span class="ss">:b</span> <span class="p">(</span><span class="nb">-&gt; </span><span class="p">(</span><span class="nf">torch.empty</span> <span class="p">(</span>, <span class="nv">f-out</span><span class="p">))</span>
         <span class="p">(</span><span class="nf">.normal_</span> <span class="ss">:mean</span> <span class="mi">0</span> <span class="ss">:std</span> <span class="mf">1.0</span><span class="p">)</span>
         <span class="p">(</span><span class="nf">Parameter</span> <span class="ss">:requires_grad</span> <span class="nv">True</span><span class="p">))))</span>
</pre></div>
</div>
</div>
<div class="section" id="reverting-models-to-s-expressions">
<h3>Reverting Models to S-Expressions<a class="headerlink" href="#reverting-models-to-s-expressions" title="Permalink to this headline">¶</a></h3>
<p>MinoTauro lets you revert models defined as mu expressions back to S-Expressions using
<code class="docutils literal notranslate"><span class="pre">hy.contrib.hy-repr</span></code>. The output model code contains mu expressions
and parameter configurations for the parent module and all nested components.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Though the current implementation (0.1.0) of model reversion works best for models which are implemented
exclusively with MinoTauro and <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>, we are working on expanding the functionality to revert any
PyTorch module into a valid S-expression.</p>
<p>The next update will work on reverting <code class="docutils literal notranslate"><span class="pre">torch.jit</span></code> scripts to S-Expressions to
help import module definitions of PyTorch models developed in native Python
to MinoTauro <cite>mu-expressions</cite>.</p>
</div>
<p>We believe having this capability will open new avenues for research in meta-learning
of computational graphs, and will streamline implementation of machine learning methods
which perform expression manipulation such as genetic programming and Sequence-to-Sequence
modeling.</p>
<p>An example of this feature on the previously defined <code class="docutils literal notranslate"><span class="pre">FeedForwardNeuralNetwork</span></code> model
is the following:</p>
<div class="highlight-clojure notranslate"><div class="highlight"><pre><span></span><span class="c1">;- Defines Model and Reverts Model To S-Expression String</span>
<span class="p">(</span><span class="nf">setv</span> <span class="nv">model</span> <span class="p">(</span><span class="nf">NeuralNetwork</span> <span class="mi">10</span> <span class="mi">32</span> <span class="mi">1</span><span class="p">))</span>
<span class="p">(</span><span class="nb">print </span><span class="p">(</span><span class="nf">hy-repr</span> <span class="nv">model</span><span class="p">))</span>

<span class="c1">;- Returns</span>
<span class="s">&quot;(bind (mu [x linear-to-hidden linear-to-output] (linear-to-output (torch.sigmoid (linear-to-hidden x))))</span>
<span class="s">       :linear-to-hidden (bind (mu [x weights bias] (+ (@ x weights) bias))</span>
<span class="s">                               :weights (torch.nn.Parameter (torch.empty (, 10 32) :dtype torch.float32))</span>
<span class="s">                               :bias (torch.nn.Parameter (torch.empty (, 32) :dtype torch.float32)))</span>
<span class="s">       :linear-to-output (bind (mu [x weights bias] (+ (@ x weights) bias))</span>
<span class="s">                               :weights (torch.nn.Parameter (torch.empty (, 32 1) :dtype torch.float32))</span>
<span class="s">                               :bias (torch.nn.Parameter (torch.empty (, 1) :dtype torch.float32))))&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="advanced-expression-threading">
<h3>Advanced Expression Threading<a class="headerlink" href="#advanced-expression-threading" title="Permalink to this headline">¶</a></h3>
<p>MinoTauro contains custom threading macros to help define more complex network
architectures. Threading macros are common to other function-heavy lisp languages such as Clojure.
Threading here refers to the practice of passing arguments through expression not concurrency.
The simplest comes in the form of the thread first macro <code class="docutils literal notranslate"><span class="pre">-&gt;</span></code>, which inserts each expression into the
next expression’s first argument place. The compliment of this macro which inserts the expressions
into the last argument place is <code class="docutils literal notranslate"><span class="pre">-&gt;&gt;</span></code>.</p>
<p>The following list lists the threading macros currently implemented
in MinoTauro. For more detailed use case information, refer to documentation for
<code class="docutils literal notranslate"><span class="pre">mino.thread</span></code>.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-&gt;</span></code> , <code class="docutils literal notranslate"><span class="pre">-&gt;&gt;</span></code> : Thread First/Last Macros</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">*-&gt;</span></code> , <code class="docutils literal notranslate"><span class="pre">*-&gt;</span></code> : Broadcast Thread First/Last Macros</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">|-&gt;</span></code> , <code class="docutils literal notranslate"><span class="pre">|-&gt;&gt;</span></code> : Inline Thread First/Last Macros</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">=-&gt;</span></code> , <code class="docutils literal notranslate"><span class="pre">=-&gt;</span></code> : Set Thread First/Last Macros</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cond-&gt;</span></code> , <code class="docutils literal notranslate"><span class="pre">cond-&gt;&gt;</span></code> : Conditional Thread First/Last Macros</p></li>
</ul>
</div>
<div class="section" id="spec-clojure-like-specifications-for-pytorch">
<h3>Spec (Clojure-like Specifications For PyTorch)<a class="headerlink" href="#spec-clojure-like-specifications-for-pytorch" title="Permalink to this headline">¶</a></h3>
<p>Inspired from Clojure’s <a class="reference external" href="https://clojure.org/about/spec">spec</a> , MinoTauro includes a similar system for predicate type checking.
This package in conjunction with the formalized <code class="docutils literal notranslate"><span class="pre">mu</span></code> allows for runtime predicate checking of components,
and other features common in Clojure’s <code class="docutils literal notranslate"><span class="pre">spec</span></code> such as <code class="docutils literal notranslate"><span class="pre">conform</span></code>, <code class="docutils literal notranslate"><span class="pre">describe</span></code>, and <code class="docutils literal notranslate"><span class="pre">gen</span></code>.
These tools were added to MintoTauro to help debug computational graphs, constrain model architecture
to facilitate design collaboration, and to easily generate valid data/models.
Here is an example of defining a data specification for the previous neural network example:</p>
<div class="highlight-clojure notranslate"><div class="highlight"><pre><span></span><span class="c1">;- Macros</span>
<span class="p">(</span><span class="nf">require</span> <span class="p">[</span><span class="nv">mino.mu</span> <span class="p">[</span><span class="nv">*</span><span class="p">]]</span>
         <span class="p">[</span><span class="nv">mino.thread</span> <span class="p">[</span><span class="nv">*</span><span class="p">]]</span>
         <span class="p">[</span><span class="nv">mino.spec</span> <span class="p">[</span><span class="nv">*</span><span class="p">]]</span>
         <span class="p">[</span><span class="nv">hy.contrib.walk</span> <span class="p">[</span><span class="nv">let</span><span class="p">]])</span>

<span class="c1">;- Imports</span>
<span class="p">(</span><span class="nb">import </span><span class="nv">torch</span>
        <span class="p">[</span><span class="nv">torch.nn.functional</span> <span class="ss">:as</span> <span class="nv">F</span><span class="p">]</span>
        <span class="p">[</span><span class="nv">torch.nn</span> <span class="ss">:as</span> <span class="nv">nn</span><span class="p">]</span>
        <span class="p">[</span><span class="nv">torch.optim</span> <span class="p">[</span><span class="nv">Adam</span><span class="p">]])</span>

<span class="c1">;-- Defines PyTorch Object Specifications</span>
<span class="p">(</span><span class="nf">spec/def</span> <span class="ss">:tensor</span> <span class="p">(</span><span class="k">fn </span><span class="p">[</span><span class="nv">x</span><span class="p">]</span> <span class="p">(</span><span class="nb">instance? </span><span class="nv">torch.Tensor</span> <span class="nv">x</span><span class="p">))</span>
          <span class="ss">:learnable</span> <span class="p">(</span><span class="k">fn </span><span class="p">[</span><span class="nv">x</span><span class="p">]</span> <span class="nv">x.requires_grad</span><span class="p">)</span>
          <span class="ss">:rank1</span> <span class="p">(</span><span class="k">fn </span><span class="p">[</span><span class="nv">x</span><span class="p">]</span> <span class="p">(</span><span class="nb">-&gt; </span><span class="nv">x</span> <span class="nv">.size</span> <span class="nv">len</span> <span class="p">(</span><span class="nb">= </span><span class="mi">1</span><span class="p">)))</span>
          <span class="ss">:rank2</span> <span class="p">(</span><span class="k">fn </span><span class="p">[</span><span class="nv">x</span><span class="p">]</span> <span class="p">(</span><span class="nb">-&gt; </span><span class="nv">x</span> <span class="nv">.size</span> <span class="nv">len</span> <span class="p">(</span><span class="nb">= </span><span class="mi">2</span><span class="p">))))</span>

<span class="c1">;-- Linear Operation</span>
<span class="p">(</span><span class="nf">defmu</span> <span class="nv">LinearTransformation</span> <span class="p">[</span><span class="nv">x</span> <span class="nv">weights</span> <span class="nv">bias</span><span class="p">]</span>
  <span class="p">(</span><span class="nb">-&gt; </span><span class="nv">x</span> <span class="p">(</span><span class="o">@</span> <span class="nv">weights</span><span class="p">)</span> <span class="p">(</span><span class="nb">+ </span><span class="nv">bias</span><span class="p">)))</span>

<span class="c1">;-- Defines a Learnable LinearTransformation data specification</span>
<span class="c1">; Weights and bias are constrained to ensure model is correctly configured.</span>
<span class="c1">; In this instance, weights must conform to Tensors with learnable gradients of ranks 2 and 1 respectively.</span>
<span class="p">(</span><span class="nf">spec/def</span> <span class="ss">:LearnableLinear</span> <span class="p">(</span><span class="nf">spec/parameters</span> <span class="nv">weights</span> <span class="p">(</span><span class="nf">spec/and</span> <span class="ss">:tensor</span> <span class="ss">:learnable</span> <span class="ss">:rank2</span><span class="p">)</span>
                                            <span class="nv">bias</span>    <span class="p">(</span><span class="nf">spec/and</span> <span class="ss">:tensor</span> <span class="ss">:learnable</span> <span class="ss">:rank1</span><span class="p">)))</span>

<span class="c1">;-- Defines a generator for the LearnableLinear Specification</span>
<span class="c1">; Generators are functions which return data that conforms to the specification.</span>
<span class="p">(</span><span class="nf">spec/defgen</span> <span class="ss">:LearnableLinear</span> <span class="p">[</span><span class="nv">f-in</span> <span class="nv">f-out</span><span class="p">]</span>
  <span class="p">(</span><span class="nf">LinearTransformation</span> <span class="ss">:weights</span> <span class="p">(</span><span class="nb">-&gt; </span><span class="p">(</span><span class="nf">torch.empty</span> <span class="p">(</span>, <span class="nv">f-in</span> <span class="nv">f-out</span><span class="p">))</span>
                                     <span class="p">(</span><span class="nf">.normal_</span> <span class="ss">:mean</span> <span class="mi">0</span> <span class="ss">:std</span> <span class="mf">1.0</span><span class="p">)</span>
                                     <span class="p">(</span><span class="nf">nn.Parameter</span> <span class="ss">:requires_grad</span> <span class="nv">True</span><span class="p">))</span>
                        <span class="ss">:bias</span>    <span class="p">(</span><span class="nb">-&gt; </span><span class="p">(</span><span class="nf">torch.empty</span> <span class="p">(</span>, <span class="nv">f-out</span><span class="p">))</span>
                                     <span class="p">(</span><span class="nf">.normal_</span> <span class="ss">:mean</span> <span class="mi">0</span> <span class="ss">:std</span> <span class="mf">1.0</span><span class="p">)</span>
                                     <span class="p">(</span><span class="nf">nn.Parameter</span> <span class="ss">:requires_grad</span> <span class="nv">True</span><span class="p">))))</span>

<span class="c1">;-- Feed Forward Operation</span>
<span class="p">(</span><span class="nf">defmu</span> <span class="nv">FeedForward</span> <span class="p">[</span><span class="nv">x</span> <span class="nv">linear-to-hidden</span> <span class="nv">linear-to-output</span><span class="p">]</span>
  <span class="p">(</span><span class="nb">-&gt; </span><span class="nv">x</span>
      <span class="nv">linear-to-hidden</span>
      <span class="nv">torch.sigmoid</span>
      <span class="nv">linear-to-output</span><span class="p">))</span>

<span class="c1">;-- Defines FeedForwardNeuralNetwork Specification</span>
<span class="p">(</span><span class="nf">spec/def</span> <span class="ss">:FeedForwardNeuralNetwork</span> <span class="p">(</span><span class="nf">spec/modules</span> <span class="nv">linear-to-hidden</span> <span class="ss">:LearnableLinear</span>
                                                  <span class="nv">linear-to-output</span> <span class="ss">:LearnableLinear</span><span class="p">))</span>

<span class="c1">;-- Defines FeedForwardNeuralNetwork Spec Generator</span>
<span class="p">(</span><span class="nf">spec/defgen</span> <span class="ss">:FeedForwardNeuralNetwork</span> <span class="p">[</span><span class="nv">nb-inputs</span> <span class="nv">nb-hidden</span> <span class="nv">nb-outputs</span><span class="p">]</span>
  <span class="p">(</span><span class="nf">FeedForward</span> <span class="ss">:linear-to-hidden</span> <span class="p">(</span><span class="nf">spec/gen</span> <span class="ss">:LearnableLinear</span> <span class="nv">nb-inputs</span> <span class="nv">nb-hidden</span><span class="p">)</span>
               <span class="ss">:linear-to-output</span> <span class="p">(</span><span class="nf">spec/gen</span> <span class="ss">:LearnableLinear</span> <span class="nv">nb-hidden</span> <span class="nv">nb-outputs</span><span class="p">)))</span>

<span class="c1">;--</span>
<span class="p">(</span><span class="nf">defmain</span> <span class="p">[</span><span class="o">&amp;</span><span class="nb">rest </span><span class="nv">_</span><span class="p">]</span>

  <span class="p">(</span><span class="nb">print </span><span class="s">&quot;Loading Model + Data...&quot;</span><span class="p">)</span>
  <span class="p">(</span><span class="k">let </span><span class="p">[</span><span class="nv">nb-inputs</span> <span class="mi">10</span> <span class="nv">nb-hidden</span> <span class="mi">32</span> <span class="nv">nb-outputs</span> <span class="mi">1</span><span class="p">]</span>

    <span class="c1">;- Defines Model + Optimizer</span>
    <span class="p">(</span><span class="nf">setv</span> <span class="nv">model</span> <span class="p">(</span><span class="nf">spec/gen</span> <span class="ss">:FeedForwardNeuralNetwork</span> <span class="nv">nb-inputs</span> <span class="nv">nb-hidden</span> <span class="nv">nb-outputs</span><span class="p">)</span>
          <span class="nv">optimizer</span> <span class="p">(</span><span class="nf">Adam</span> <span class="p">(</span><span class="nf">.parameters</span> <span class="nv">model</span><span class="p">)</span> <span class="ss">:lr</span> <span class="mf">0.001</span> <span class="ss">:weight_decay</span> <span class="mi">1</span><span class="nv">e-5</span><span class="p">))</span>

    <span class="c1">;- Generates Dummy Data</span>
    <span class="p">(</span><span class="k">let </span><span class="p">[</span><span class="nv">batch-size</span> <span class="mi">100</span><span class="p">]</span>
      <span class="p">(</span><span class="nf">setv</span> <span class="nv">x</span> <span class="p">(</span><span class="nb">-&gt; </span><span class="p">(</span><span class="nf">torch.empty</span> <span class="p">(</span>, <span class="nv">batch-size</span> <span class="nv">nb-inputs</span><span class="p">))</span>
                  <span class="p">(</span><span class="nf">.normal_</span> <span class="ss">:mean</span> <span class="mi">0</span> <span class="ss">:std</span> <span class="mf">1.0</span><span class="p">))</span>
            <span class="nv">y</span> <span class="p">(</span><span class="nf">torch.ones</span> <span class="p">(</span>, <span class="nv">batch-size</span> <span class="nv">nb-outputs</span><span class="p">)))))</span>

  <span class="c1">;- Train</span>
  <span class="p">(</span><span class="k">let </span><span class="p">[</span><span class="nv">epochs</span> <span class="mi">100</span><span class="p">]</span>
    <span class="p">(</span><span class="nb">print </span><span class="s">&quot;Training...&quot;</span><span class="p">)</span>
    <span class="p">(</span><span class="nb">for </span><span class="p">[</span><span class="nv">epoch</span> <span class="p">(</span><span class="nb">range </span><span class="nv">epochs</span><span class="p">)]</span>

      <span class="c1">;- Forward</span>
      <span class="p">(</span><span class="nf">setv</span> <span class="nv">y-pred</span> <span class="p">(</span><span class="nf">model</span> <span class="nv">x</span><span class="p">))</span>
      <span class="p">(</span><span class="nf">setv</span> <span class="nv">loss</span> <span class="p">(</span><span class="nf">F.binary_cross_entropy_with_logits</span> <span class="nv">y-pred</span> <span class="nv">y</span><span class="p">))</span>
      <span class="p">(</span><span class="nb">print </span><span class="p">(</span><span class="nf">.format</span> <span class="s">&quot;Epoch: {epoch} Loss: {loss}&quot;</span> <span class="ss">:epoch</span> <span class="nv">epoch</span> <span class="ss">:loss</span> <span class="nv">loss</span><span class="p">))</span>

      <span class="c1">;- Backward</span>
      <span class="p">(</span><span class="nf">.zero_grad</span> <span class="nv">optimizer</span><span class="p">)</span>
      <span class="p">(</span><span class="nf">.backward</span> <span class="nv">loss</span><span class="p">)</span>
      <span class="p">(</span><span class="nf">.step</span> <span class="nv">optimizer</span><span class="p">))))</span>
</pre></div>
</div>
<p>The neural network example uses <code class="docutils literal notranslate"><span class="pre">mino.spec</span></code> to
define valid configurations of the <code class="docutils literal notranslate"><span class="pre">LearnableLinear</span></code> and <code class="docutils literal notranslate"><span class="pre">FeedForwardNeuralNetwork</span></code> modules.
These <code class="docutils literal notranslate"><span class="pre">mino.spec</span></code> definitions makes it simple to test that modules
have valid components. If a generator is defined for a <code class="docutils literal notranslate"><span class="pre">mino.spec</span></code>, then
the generated data will be tested against its specification and fail when
the data does not conform.</p>
</div>
</div>
<div class="section" id="getting-started">
<h2>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h2>
<p>You can install the latest stable version of MinoTauro (0.1.0) by running:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ pip3 install git+https://github.com/rz4/MinoTauro.git
</pre></div>
</div>
<div class="section" id="jupyter-notebook-running-minotauro">
<h3>Jupyter Notebook Running MinoTauro<a class="headerlink" href="#jupyter-notebook-running-minotauro" title="Permalink to this headline">¶</a></h3>
<p>If you prefer to develop models in a Jupyter Notebook, you can
try out <strong>Calysto</strong>, a Hy Kernel for Jupyter. The MinoTauro GitHub
Repo contains a up to date version for Hy 0.19.0 and information
on how to install it. Click <a class="reference external" href="https://github.com/rz4/MinoTauro#jupyter-notebook-setup">here</a> for more information.</p>
</div>
<div class="section" id="tutorials-api">
<h3>Tutorials &amp; API<a class="headerlink" href="#tutorials-api" title="Permalink to this headline">¶</a></h3>
<p>We are starting to compile some tutorials which show how to use MinoTauro
to build PyTorch models with ease. For more in depth explanations of
each macro provided in MinoTauro, refer to the <code class="docutils literal notranslate"><span class="pre">mino.mu</span></code>, <code class="docutils literal notranslate"><span class="pre">mino.thread</span></code>
and <code class="docutils literal notranslate"><span class="pre">mino.spec</span></code> APIs respectively.</p>
<div class="toctree-wrapper compound">
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="tutorials/index.html" class="btn btn-neutral float-right" title="tutorials" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="MinoTauro (0.1.0) API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Rafael Zamora-Resendiz

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>