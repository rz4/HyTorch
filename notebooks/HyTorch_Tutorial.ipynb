{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LOGO](logo.png)\n",
    "# (eval '(HyTorch Tutorial))\n",
    "Introduction to PyTorch Meta-Programming Using the Lisp Dialect Hy\n",
    "\n",
    "Lead Maintainer: [Rafael Zamora-Resendiz](https://github.com/rz4)\n",
    "\n",
    "**HyTorch** is a Hy (0.16.0) library running Python (3.7) and PyTorch (1.0.1)\n",
    "for use in rapid low-level development of deep learning (DL) systems as well as\n",
    "for experiments in DL meta-programming.\n",
    "\n",
    "##### Table of Contents\n",
    "1. [Motivation](#s1)\n",
    "2. [Installation](#s2)\n",
    "3. [Hy: Lisp Flavored Python](#s3)\n",
    "4. [HyTorch in Action](#s4)\n",
    "    1. [PyTorch Models as Hy-Expressions](#s41)\n",
    "    2. [Expression Threading](#s42)\n",
    "    3. [Pattern Matching](#s43)\n",
    "    4. [Expression Refactoring](#s44)\n",
    "5. [Network Analysis and Meta-Analysis Using HyTorch](#s5)\n",
    "    1. [FUTURE: Fetching Internal Network Components](#s51)\n",
    "    2. [FUTURE: Probing Networks Using Tests](#s52)\n",
    "    3. [FUTURE: Loading Foriegn Pytorch Models](#s53)\n",
    "    4. [FUTURE: Comparing Network Architectures](#s54)\n",
    "6. [FUTURE: Hyper-Parameter Search Using Genetic Programming]()\n",
    "    \n",
    "---\n",
    "\n",
    "<a name=\"s1\"></a>\n",
    "## Motivation\n",
    "The dynamic execution of PyTorch operations allows enough flexibility to change\n",
    "computational graphs on the fly. This provides an avenue for Hy, a lisp-binding\n",
    "library for Python, to be used in establishing meta-programming practices in the\n",
    "field of deep learning.\n",
    "\n",
    "While the final goal of this project is to build a framework for DL systems to have\n",
    "access to their own coding, this coding paradigm also shows promise at accelerating the development of new deep learning models while allowing significant manipulation of low-torch tensor operations at runtime. A common trend in current DL packages is an abundance of object-oriented abstraction with packages such as Keras. This only reduces transparity to the already black-box nature of NN systems, and makes interpretability and reproducibility of models even more difficult.\n",
    "\n",
    "In order to better understand NN models and allow for quick iterative design\n",
    "over novel or esoteric architectures, a deep learning programmer requires access to an\n",
    "environment that allows low-level definition of tensor graphs and provides methods to quickly access network components for analysis, while still providing a framework to manage large architectures. I believe that the added expressability of Lisp in combination with PyTorch's functional API allows for this type of programming paradigm, and provides DL researchers an extendable framework which cannot be matched by any other abstracted NN packages.\n",
    "\n",
    "<a name=\"s2\"></a>\n",
    "## Installation\n",
    "\n",
    "The current project has been tested using Hy 0.16.0, PyTorch 1.0.1.post2 and\n",
    "Python 3.7. The following ***Pip*** command can be used to install **HyTorch**:\n",
    "\n",
    "```\n",
    "$ pip3 install git+https://github.com/rz4/HyTorch\n",
    "```\n",
    "---\n",
    "\n",
    "<a name=\"s3\"></a>\n",
    "## Hy: Lisp Flavored Python\n",
    "\n",
    "\"Hy is a dialect of the language Lisp designed to interact with Python by translating expressions into Python's abstract syntax tree (AST). Similar to Clojure's mapping of s-expressions onto the Java virtual machine (JVM), Hy is meant to operate as a transparent Lisp front end to Python. Lisp allows operating on code as data (metaprogramming). Thus, Hy can be used to write domain-specific languages. Hy also allows Python libraries, including the standard library, to be imported and accessed alongside Hy code with a compiling step converting the data structure of both into Python's AST.\" [Source: Wikipedia]()\n",
    "\n",
    "It's recommended to look over [Hy's tutorial](http://docs.hylang.org/en/stable/tutorial.html) as it does a good job showcasing the various features of Hy. In short, Hy provides a Python-friendly Lisp which anyone who knows Python can easy pickup. Plus, you can import any Python code into Hy as well as importing Hy code to Python! Here is just a little taste of how Hy is structured: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello FooManCHEW ! It's a great day to be Lisping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Lisp-style Comments\n",
    "\n",
    ";; Define function hello-world\n",
    "(defn hello-world [name] (print \"Hello\" name \"! It's a great day to be Lisping!\"))\n",
    "\n",
    ";; Evaluate\n",
    "(hello-world \"FooManCHEW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyExpression([\n",
      "  HySymbol('+'),\n",
      "  HyExpression([\n",
      "    HySymbol('+'),\n",
      "    HyInteger(1),\n",
      "    HyInteger(2)]),\n",
      "  1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Importing Numpy\n",
    "(import [numpy :as np])\n",
    "\n",
    "; Still be able to access attribute functions using dot notation\n",
    "(setv x (np.ones '(10 10)))\n",
    "\n",
    "; Quasi-quote and unquote\n",
    "(print `(+ (+ 1 2) ~(- 4 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"s4\"></a>\n",
    "## HyTorch In Action\n",
    "\n",
    "<a name=\"s41\"></a>\n",
    "### PyTorch Models as Hy-Expressions:\n",
    "\n",
    "First, let's load the necessary packages from HyTorch and Pytorch. We also will be setting our device to available resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, True, True, None, None, None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Importing HyTorch Tools\n",
    "(import [hytorch.core [|gensym print-lisp]])\n",
    "(require [hytorch.core [|setv]])\n",
    "(require [hytorch.thread [|-> |->> *-> *->>]])\n",
    "\n",
    "; Import PyTorch\n",
    "(import torch)\n",
    "(import [torch.nn.functional :as tfun])\n",
    "\n",
    "; Checking for available cuda device\n",
    "(setv device (torch.device (if (.is_available torch.cuda) \"cuda:0\" \"cpu\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a list of leaf tensors which will be our trainable parameters in the network. Hy allows us to write the leaf tensor defintions as Hy-expressions and store the code in a unevaluated list. We then generate a list of symbols that will be mapped to the tensors using `|gensym` and define an expression to assign the evaluated leaf tensor definitions to the list of generated symbols using `|setv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[L_0 L_1 L_2 L_3 L_4] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Defining leaf tensors and variable names\n",
    "(setv leaf-tensor-defs '[(torch.empty [10] :dtype torch.float32 :requires-grad True)\n",
    "                         (torch.empty [10 10] :dtype torch.float32 :requires-grad True)\n",
    "                         (torch.empty [10] :dtype torch.float32 :requires-grad True)\n",
    "                         (torch.empty [1 10] :dtype torch.float32 :requires-grad True)\n",
    "                         (torch.empty [1] :dtype torch.float32 :requires-grad True)])\n",
    "\n",
    "; Generate symbols for tensor-defs\n",
    "(setv leaf-tensors (|gensym leaf-tensor-defs \"L_\"))\n",
    "\n",
    "; Define assign expression for leafs\n",
    "(setv create-leafs `(|setv ~leaf-tensors ~leaf-tensor-defs))\n",
    "\n",
    "; Print reference symbols\n",
    "(print-lisp leaf-tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our leaf tensors are empty at the momement, let's initialize each according to a random normal distribution and push each to our computing device. Again, the procedure for each leaf tensor can be defined in a list of unevaluated Hy-expressions. We can then apply these procedures by threading them to our leaf tensor symbols and store this expression for later use (thread macros are explained in the next section). Finally, we generate a new set of symbols for the initialized weights and define a setter expression for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W_0 W_1 W_2 W_3 W_4] \n",
      "[(.to (torch.nn.init.normal L_0) device) (.to (torch.nn.init.normal L_1) device) (.to (torch.nn.init.normal L_2) device) (.to (torch.nn.init.normal L_3) device) (.to (torch.nn.init.normal L_4) device)] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Define intialization procedures\n",
    "(setv tensor-inits '[(-> torch.nn.init.normal (.to device))\n",
    "                     (-> torch.nn.init.normal (.to device))\n",
    "                     (-> torch.nn.init.normal (.to device))\n",
    "                     (-> torch.nn.init.normal (.to device))\n",
    "                     (-> torch.nn.init.normal (.to device))])\n",
    "\n",
    "; Define init procedure application to leafs\n",
    "(setv init-leafs (macroexpand `(|-> ~leaf-tensors ~tensor-inits)))\n",
    "\n",
    "; Generate symbols for init weights\n",
    "(setv w-tensors (|gensym leaf-tensor-defs \"W_\"))\n",
    "\n",
    "; Define assign expression for weights\n",
    "(setv init-weights `(|setv ~w-tensors ~init-leafs))\n",
    "\n",
    "; Print\n",
    "(print-lisp w-tensors)\n",
    "(print-lisp init-leafs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define the network as a seperate expression. Notice the threading macro `->`, which takes the first argument and places it as the first argument to the next argument in the series. This is a prebuilt threading macro in Hy and its very useful in defining long functional expressions in an inline format. The resulting expression is very clean and easy to follow. Thanks to PyTorch's functional API, we can take full advantage of threading macros for our network definitions. The next section will talk more about threading and the custom threading macros provided in HyTorch for more complex network designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-> W_0 (tfun.linear W_1 W_2) tfun.sigmoid (tfun.linear W_3 W_4) tfun.sigmoid) \n",
      "(tfun.sigmoid (tfun.linear (tfun.sigmoid (tfun.linear W_0 W_1 W_2)) W_3 W_4)) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Defining a simple feed-forward NN as an S-Expression\n",
    "(setv nn-def '(-> W_0 \n",
    "                  (tfun.linear W_1 W_2) \n",
    "                  tfun.sigmoid \n",
    "                  (tfun.linear W_3 W_4) \n",
    "                  tfun.sigmoid))\n",
    "\n",
    "; Print\n",
    "(print-lisp nn-def)\n",
    "(print-lisp (macroexpand nn-def))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macros are powerful tools and allow for the extension of the source langauge into one that more accomadates the problem space. Here, we define a new macro which returns an expression for our parameter initialization routine.\n",
    "By defining is procedure as a macro, the evaluated S-expressions will have access to the global variables defined in the notebook already. This macro can be later called to reset the network as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function init_params at 0x115db17b8>, [None, None, None, None, None]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Define network parameter init procedure\n",
    "(defmacro init-params []\n",
    "  '(do (eval create-leafs) \n",
    "       (eval init-weights)))\n",
    "\n",
    "; Initiate Parameters\n",
    "(init-params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run forward propagation of the network graph by simply evaluating our network expression and storing the resulting output in a new variable. By keeping model components seperate from one another, we have more modular control over what is being executed making debugging and refactoring much easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3209], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Running Forward Prop\n",
    "(setv out (eval nn-def))\n",
    "(print out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"s42\"></a>\n",
    "### Expression Threading:\n",
    "Hy has some pre-built threading macros to help write nested functions in inline\n",
    "notation. This is a great start, but can be improved with some more advanced features to keep with inline notation while providing argument broadcasting and multidimensional threading for more complex computational graphs. \n",
    "\n",
    "Let's first review the two threading macros native to Hy, those being `->` and `->>`. These thread the arguments into each other's first or last argument slot respectively. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(print (* (+ 10 2) 5)) \n",
      "(print (* 5 (+ 2 10))) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Head Threading\n",
    "(print-lisp (macroexpand '(-> 10 (+ 2) (* 5) print)))\n",
    "\n",
    "; Tail Threading\n",
    "(print-lisp (macroexpand '(->> 10 (+ 2) (* 5) print)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great for defining sequential computational graphs, but it becomes a bit messy when dealing with branching architectures. For these cases, we can use the broadcasting threading macros `*->` and `*->>`. This set of macros allows for arquitectures with multiple input/outputs or branching intermediate layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tfun.sigmoid (tfun.add (tfun.matmul input1 input2) bias)) (tfun.relu (tfun.add (tfun.matmul input1 input2) bias))] \n",
      "[(tfun.sigmoid (tf.add bias (tfun.matmul input1 input2))) (tfun.relu (tf.add bias (tfun.matmul input1 input2)))] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Head Broadcast Threading\n",
    "(print-lisp (macroexpand '(*-> [input1 input2] tfun.matmul (tfun.add bias) [tfun.sigmoid tfun.relu])))\n",
    "\n",
    "; Tail Broadcast Threading\n",
    "(print-lisp (macroexpand '(*->> [input1 input2] tfun.matmul (tf.add bias) [tfun.sigmoid tfun.relu])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a list is threaded into the next argument, the list will be place as the first or last N arguments depending on head or tail threading. When the next argument is a list, the previous argument will be broadcasted to all elements in the next argument. In the previous example, we fed two inputs tensors, operated with matrix multiplication, added a bias tensor, and then outputed two tensors with different activation applied to them. \n",
    "\n",
    "There was another special threading macro used in the previous section called `|->` (with its tail counterpart being `|->>`). These macros are used for inline threading of lists. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tfun.sigmoid (tfun.linear input1 w1 b1)) (tfun.sigmoid (tf.linear input2 w2 b2))] \n",
      "[(tfun.sigmoid (tfun.linear w1 b1 input1)) (tfun.sigmoid (tf.linear w2 b2 input2))] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Head List Inline Threading\n",
    "(print-lisp (macroexpand '(|-> [input1 input2] [(tfun.linear w1 b1) (tf.linear w2 b2)] tfun.sigmoid)))\n",
    "\n",
    "; Tail List Inline Threading\n",
    "(print-lisp (macroexpand '(|->> [input1 input2] [(tfun.linear w1 b1) (tf.linear w2 b2)] tfun.sigmoid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we made two nueral networks that function in parallel. We applied the same final activation to both, but only needed to define it once. These macros were used in the previous section to apply the initialization operators on each leaf tensor. Using these macros, complex arquitectures can be easily defined while maintaining a readable inline syntax. Since these are macros, the expanded code are legal Hy-expressions which can be operated on by different components of your program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"s43\"></a>\n",
    "### Pattern Matching:\n",
    "Because network definitions are written as Hy-expressions, we can perform pattern matching to find internal components of an architecture. HyTorch contains pattern-matching tools which can be used test if two `HyExpressions` are equal with class-based matching denoted by `HyKeywords`. We can test to see if an expression matches a pattern as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, True]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Import Pattern Matching Functions and Macros\n",
    "(import [hytorch.match [pat-match? pat-find]])\n",
    "(require [hytorch.match [pat-refract]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; pat-match? expr pattern\n",
    "(pat-match? '(print (+ (+ 2 2) 3)) '(print (+ :HyExpression 3)))\n",
    "\n",
    "; Match by parent-class association (ex. hy.model.HyExpression child of hy)\n",
    "(pat-match? '(print (+ (+ 1 2) 3)) '(print (+ :hy 3)))\n",
    "\n",
    "; Match by sub classes (ex. Hy.models)\n",
    "(pat-match? '(print (+ (+ 1 2) 3)) '(print (+ :hy:models 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pattern matching, we can perform searches on our network defintions and retrieve subcomponents of the architecture. Here, we take the network defintion from the previous section and try to match portions of the network with a desired pattern. We match for expressions that start with sigmoid activation and are followed by a Hy expression. We first exapand the macro'd defintion and then look for the first 2 results. The matches are returned with the shallowest matches first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Match:\n",
      "(tfun.sigmoid (tfun.linear (tfun.sigmoid (tfun.linear W_0 W_1 W_2)) W_3 W_4)) \n",
      "Sigmoid Match:\n",
      "(tfun.sigmoid (tfun.linear W_0 W_1 W_2)) \n",
      "Linear Match:\n",
      "(tfun.linear (tfun.sigmoid (tfun.linear W_0 W_1 W_2)) W_3 W_4) \n",
      "Linear Match:\n",
      "(tfun.linear W_0 W_1 W_2) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Our network from the previous section\n",
    "(setv nn-def '(-> W_0 \n",
    "                  (tfun.linear W_1 W_2) \n",
    "                  tfun.sigmoid \n",
    "                  (tfun.linear W_3 W_4) \n",
    "                  tfun.sigmoid))\n",
    "\n",
    "; Pattern match search over expanded defintion and return first 2\n",
    "(for [match (pat-find (macroexpand nn-def) '(tfun.sigmoid :HyExpression) :n 2)]\n",
    "     (print \"Sigmoid Match:\")\n",
    "     (print-lisp match))\n",
    "\n",
    "; Find patterns using the &rest keyword. \n",
    "(for [match (pat-find (macroexpand nn-def) '(tfun.linear &rest) :n 2)]\n",
    "     (print \"Linear Match:\")\n",
    "     (print-lisp match))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"s44\"></a>\n",
    "### Hy-Expression Refactoring:\n",
    "With pattern matching, we can also quickly refactor components from our network arquitecture without having to manually rewrite any of the orginal code. This is very useful when experimenting with different hyper parameters and while keeping track of previously explored designs. HyTorch implements this using the following function which takes in pair of arguments defining the searched expression and the desired refactored code as expression tuples. These tuples have the pattern for the first element and the code that will replace the pattern as the second element. Lets change those activations to Relu and change out weight reference symbol `W_2` and `W_4` for `B_2` and `B_4` on the same model definiton. We can also match using the pattern matching kewords and @rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tfun.relu (tfun.linear (tfun.relu (tfun.linear W_0 W_1 B_2)) W_3 B_4)) \n",
      "(tfun.relu (tfun.conv2d W_0)) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Refactor model definition\n",
    "(print-lisp (macroexpand `(pat-refract ~(macroexpand nn-def) (tfun.sigmoid tfun.relu) \n",
    "                                                             (W_2 B_2) \n",
    "                                                             (W_4 B_4))))\n",
    "\n",
    "; Refactor model using pattern matching forms\n",
    "(print-lisp (macroexpand `(pat-refract ~(macroexpand nn-def) (tfun.sigmoid tfun.relu) \n",
    "                                                             ((tfun.linear &rest) (tfun.conv2d W_0))))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"s5\"></a>\n",
    "## Network Analysis and Meta-Analysis Using HyTorch\n",
    "\n",
    "<a name=\"s51\"></a>\n",
    "### Fetching Internal Network Components:\n",
    "\n",
    "When working on model intepretation ande debuging its important to be able to fetch internal components of the model. While it's simple getting layer outputs by using the pattern search tool and then evaluating the sub-expression, how do we get gradients for tensors not defined as our leafs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"s52\"></a>\n",
    "### Probing Networks Using Tests:\n",
    "\n",
    "<a name=\"s53\"></a>\n",
    "### Loading Foreign Pytorch Models:\n",
    "\n",
    "<a name=\"s54\"></a>\n",
    "### Comparing Network Architectures:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Calysto Hy",
   "language": "hy",
   "name": "calysto_hy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "scheme"
   },
   "mimetype": "text/x-hylang",
   "name": "hy",
   "pygments_lexer": "lisp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
