{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LOGO](logo.png)\n",
    "# (eval '(HyTorch Tutorial))\n",
    "Introduction to PyTorch Meta-Programming Using the Lisp Dialect Hy\n",
    "\n",
    "Lead Maintainer: [Rafael Zamora-Resendiz](https://github.com/rz4)\n",
    "\n",
    "**HyTorch** is a Hy (0.16.0) library running Python (3.7) and PyTorch (1.0.1)\n",
    "for use in rapid low-level development of deep learning (DL) systems as well as\n",
    "for experiments in DL meta-programming.\n",
    "\n",
    "##### Table of Contents\n",
    "1. [Motivation](#s1)\n",
    "2. [Installation](#s2)\n",
    "3. [Hy: Lisp Flavored Python](#s3)\n",
    "4. [Hytorch in Action](#s4)\n",
    "    1. [Pytorch Models as S-Expressions](#s41)\n",
    "    2. [S-Expression Threading](#s42)\n",
    "    3. [Pattern Matching](#s43)\n",
    "    4. [S-Expression Refactoring](#s44)\n",
    "5. [Network Analysis and Meta-Analysis Using HyTorch](#s5)\n",
    "    1. [FUTURE: Fetching Internal Network Components](#s51)\n",
    "    2. [FUTURE: Probing Networks Using Tests](#s52)\n",
    "    3. [FUTURE: Loading Foriegn Pytorch Models](#s53)\n",
    "    4. [FUTURE: Comparing Network Architectures](#s54)\n",
    "6. [FUTURE: Hyper-Parameter Search Using Genetic Programming]()\n",
    "    \n",
    "---\n",
    "\n",
    "<a name=\"s1\"></a>\n",
    "## Motivation\n",
    "The dynamic execution of PyTorch operations allows enough flexibility to change\n",
    "computational graphs on the fly. This provides an avenue for Hy, a lisp-binding\n",
    "library for Python, to be used in establishing meta-programming practices in the\n",
    "field of deep learning.\n",
    "\n",
    "While the final goal of this project is to build a framework for DL systems to have\n",
    "access to their own coding, this coding paradigm also shows promise at accelerating the development of new deep learning models while providing significant access to low-torch tensor operations at runtime. A common trend in current DL packages is an abundance of object-oriented abstraction with packages such as Keras. This only reduces transparity to the already black-box nature of NN systems, and makes interpretability and reproducibility of models even more difficult.\n",
    "\n",
    "In order to better understand NN models and allow for quick iterative design\n",
    "over novel or esoteric architectures, a deep learning programmer requires access to an\n",
    "environment that allows low-level definition of tensor graphs and provides methods to quickly access network components for analysis, while still providing a framework to manage large architectures. I believe that the added expressability of Lisp in combination with PyTorch's functional API allows for this type of programming paradigm, and provides DL researchers an extendable framework which cannot be matched by any other abstracted NN packages.\n",
    "\n",
    "<a name=\"s2\"></a>\n",
    "## Installation\n",
    "\n",
    "The current project has been tested using Hy 0.16.0, PyTorch 1.0.1.post2 and\n",
    "Python 3.7. The following ***Pip*** command can be used to install **HyTorch**:\n",
    "\n",
    "```\n",
    "$ pip3 install git+https://github.com/rz4/HyTorch\n",
    "```\n",
    "---\n",
    "\n",
    "<a name=\"s3\"></a>\n",
    "## Hy: Lisp Flavored Python\n",
    "\n",
    "\"Hy is a dialect of the language Lisp designed to interact with Python by translating expressions into Python's abstract syntax tree (AST). Similar to Clojure's mapping of s-expressions onto the Java virtual machine (JVM), Hy is meant to operate as a transparent Lisp front end to Python. Lisp allows operating on code as data (metaprogramming). Thus, Hy can be used to write domain-specific languages. Hy also allows Python libraries, including the standard library, to be imported and accessed alongside Hy code with a compiling step converting the data structure of both into Python's AST.\" [Source: Wikipedia]()\n",
    "\n",
    "I recommend looking over the [Hy documentation](http://docs.hylang.org/en/stable/tutorial.html) as they do a good job showcasing the various features of Hy. In short, Hy provides a Python-friendly Lisp which anyone who knows Python can easy pickup. Plus, you can import any Python code into Hy as well as importing Hy code to Python! Here is just a little tast of how Hy looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello FooManCHEW ! It's a great day to be Lisping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Lisp-style Comments\n",
    "\n",
    ";; Define function hello-world\n",
    "(defn hello-world [name] (print \"Hello\" name \"! It's a great day to be Lisping!\"))\n",
    "\n",
    ";; Evaluate\n",
    "(hello-world \"FooManCHEW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyExpression([\n",
      "  HySymbol('+'),\n",
      "  HyExpression([\n",
      "    HySymbol('+'),\n",
      "    HyInteger(1),\n",
      "    HyInteger(2)]),\n",
      "  1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Importing Numpy\n",
    "(import [numpy :as np])\n",
    "\n",
    "; Still be able to access attribute functions using dot notation\n",
    "(setv x (np.ones '(10 10)))\n",
    "\n",
    "; Quasi-quote and unquote\n",
    "(print `(+ (+ 1 2) ~(- 4 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"s4\"></a>\n",
    "## HyTorch In Action\n",
    "\n",
    "<a name=\"s41\"></a>\n",
    "### Pytorch Models as S-Expressions:\n",
    "\n",
    "First, let's load the nessasary packages from HyTorch and Pytorch. We also will be setting our device to the available resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, True, True, None, None, None, None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Importing Hytorch Tools\n",
    "(import [hytorch.core [|gensym]])\n",
    "(require [hytorch.core [|setv]])\n",
    "(require [hytorch.thread [*]])\n",
    "(import [hytorch.lisp [printlisp]])\n",
    "\n",
    "; Import PyTorch\n",
    "(import torch)\n",
    "(import [torch.nn.functional :as tfun])\n",
    "\n",
    "; Checking for available cuda device\n",
    "(setv device (torch.device (if (.is_available torch.cuda) \"cuda:0\" \"cpu\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a list of leaf tensors which will be our trainable parameters in the network. Hy allows us to write the leaf tensor defintions as S-expressions and store the code in a unevaluated list. We then generate a list of symbols that will be mapped to the tensors and define an expression to assign the evaluated leaf tensor definitions to the list of generated symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[L_0 L_1 L_2 L_3 L_4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Defining leaf tensors and variable names\n",
    "(setv leaf-tensor-defs '[(torch.empty [10] :dtype torch.float32 :requires-grad True)\n",
    "                         (torch.empty [10 10] :dtype torch.float32 :requires-grad True)\n",
    "                         (torch.empty [10] :dtype torch.float32 :requires-grad True)\n",
    "                         (torch.empty [1 10] :dtype torch.float32 :requires-grad True)\n",
    "                         (torch.empty [1] :dtype torch.float32 :requires-grad True)])\n",
    "\n",
    "; Generate symbols for tensor-defs\n",
    "(setv leaf-tensors (|gensym leaf-tensor-defs \"L_\"))\n",
    "\n",
    "; Define assign expression for leafs\n",
    "(setv create-leafs `(|setv ~leaf-tensors ~leaf-tensor-defs))\n",
    "\n",
    "; Print reference symbols\n",
    "(printlisp leaf-tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our leaf tensors are empty at the momement, let's initialize them according to a random normal distribution and push them to our computing device. Again, the procedure for each leaf tensor can be defined in a list of unevaluated S-expressions. We can then apply these procedures by threading them to our leaf tensor symbols and store this expression for later use. Finally, we generate a new set of symbols for the initialized weights and define a set expression for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W_0 W_1 W_2 W_3 W_4]\n",
      "[(.to (torch.nn.init.normal L_0) device) (.to (torch.nn.init.normal L_1) device) (.to (torch.nn.init.normal L_2) device) (.to (torch.nn.init.normal L_3) device) (.to (torch.nn.init.normal L_4) device)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Define intialization procedures\n",
    "(setv tensor-inits '[(-> torch.nn.init.normal (.to device))\n",
    "                     (-> torch.nn.init.normal (.to device))\n",
    "                     (-> torch.nn.init.normal (.to device))\n",
    "                     (-> torch.nn.init.normal (.to device))\n",
    "                     (-> torch.nn.init.normal (.to device))])\n",
    "\n",
    "; Define init procedure application to leafs\n",
    "(setv init-leafs (macroexpand `(|-> ~leaf-tensors ~tensor-inits)))\n",
    "\n",
    "; Generate symbols for init weights\n",
    "(setv w-tensors (|gensym leaf-tensor-defs \"W_\"))\n",
    "\n",
    "; Define assign expression for weights\n",
    "(setv init-weights `(|setv ~w-tensors ~init-leafs))\n",
    "\n",
    "; Print\n",
    "(printlisp w-tensors)\n",
    "(printlisp init-leafs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define the network as a seperate expression. Notice the threading macro `->`, which takes the first argument and places it as the first argument to the next argument in the series. This is a prebuilt threading macro in Hy and its very useful in defining long functional expressions in a inline format. The resulting expression is very clean and easy to follow. Thanks to PyTorch's functional API, we can take full advantage of threading macros for our network definitions. The next section will talk more about threading and the custom threading macros provided in HyTorch for more complex network designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-> W_0 (tfun.linear W_1 W_2) tfun.sigmoid (tfun.linear W_3 W_4) tfun.sigmoid)\n",
      "(tfun.sigmoid (tfun.linear (tfun.sigmoid (tfun.linear W_0 W_1 W_2)) W_3 W_4))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Defining a simple feed-forward NN as an S-Expression\n",
    "(setv nn-def '(-> W_0 \n",
    "                  (tfun.linear W_1 W_2) \n",
    "                  tfun.sigmoid \n",
    "                  (tfun.linear W_3 W_4) \n",
    "                  tfun.sigmoid))\n",
    "\n",
    "; Print\n",
    "(printlisp nn-def)\n",
    "(printlisp (macroexpand nn-def))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macros are powerful tools and allow for the extension of the source langauge into one that more accomadates the problem space. Here, we define a new macro which returns an expression for our parameter initialization routine.\n",
    "This can be later called on to reset the network as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function init_params at 0x1198a6d90>, [None, None, None, None, None]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Define network parameter init procedure\n",
    "(defmacro init-params []\n",
    "  '(do (eval create-leafs) \n",
    "       (eval init-weights)))\n",
    "\n",
    "; Initiate Parameters\n",
    "(init-params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run forward propagation of the network graph by simply evaluating our network expression and storing the resulting output in a new variable. By keeping model components seperate from one another, we have more modular control over what is being executed making debugging and refactoring much easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9154], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Running Forward Prop\n",
    "(setv out (eval nn-def))\n",
    "(print out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"s42\"></a>\n",
    "### S-Expression Threading:\n",
    "Hy has some pre-built threading macros to help write nested functions in inline\n",
    "notation. This is a great start, but can be imporved with some more advanced features to keep with inline notation while providing argument broadcasting and multidimensional threading for more complex computational graphs. \n",
    "\n",
    "Let's first review the two threading macros native to Hy, those being `->` and `->>`. These thread the arguments into each other's first or last argument slot repsectively. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(print (* (+ 10 2) 5))\n",
      "(print (* 5 (+ 2 10)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Head Threading\n",
    "(printlisp (macroexpand '(-> 10 (+ 2) (* 5) print)))\n",
    "\n",
    "; Tail Threading\n",
    "(printlisp (macroexpand '(->> 10 (+ 2) (* 5) print)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great for defining sequential computational graphs, but it becomes a bit messy when dealing with branching architectures. For these cases, we can use the broadcasting threading macros `*->` and `*->>`. This set of macros allows for arquitectures with multiple input/outputs or branching intermediate layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tfun.sigmoid (tfun.add (tfun.matmul input1 input2) bias)) (tfun.relu (tfun.add (tfun.matmul input1 input2) bias))]\n",
      "[(tfun.sigmoid (tf.add bias (tfun.matmul input1 input2))) (tfun.relu (tf.add bias (tfun.matmul input1 input2)))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Head Broadcast Threading\n",
    "(printlisp (macroexpand '(*-> [input1 input2] tfun.matmul (tfun.add bias) [tfun.sigmoid tfun.relu])))\n",
    "\n",
    "; Tail Broadcast Threading\n",
    "(printlisp (macroexpand '(*->> [input1 input2] tfun.matmul (tf.add bias) [tfun.sigmoid tfun.relu])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a list is threaded into the next argument, the list will be place as the first or last N arguments depending on the macro. When the next argument is a list, the previous argument will be broadcasted to all elements in the next argument. In the previous example, we fed two inputs tensors, operated with matric multiplication, added a bias tensor, and then outputed two tensors with different activation applied to them. \n",
    "\n",
    "There was another special threading macro used in the previous section called `|->` (with its tail counterpart being `|->>`). These macros are used for inline threading of lists. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tfun.sigmoid (tfun.linear input1 w1 b1)) (tfun.sigmoid (tf.linear input2 w2 b2))]\n",
      "[(tfun.sigmoid (tfun.linear w1 b1 input1)) (tfun.sigmoid (tf.linear w2 b2 input2))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Head List Inline Threading\n",
    "(printlisp (macroexpand '(|-> [input1 input2] [(tfun.linear w1 b1) (tf.linear w2 b2)] tfun.sigmoid)))\n",
    "\n",
    "; Tail List Inline Threading\n",
    "(printlisp (macroexpand '(|->> [input1 input2] [(tfun.linear w1 b1) (tf.linear w2 b2)] tfun.sigmoid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we made two nueral networks that function in parallel. We applied the same final activation to both, but only needed to define it once. These macros were used in the previous section to apply the initialization operators on each leaf tensor. Using these macros, complex arquitectures can be easily defined while maintaining a readable inline syntax. Since these are macros, the outputs are legal S-expressions which can be operated on by different components of your program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"s43\"></a>\n",
    "### Pattern Matching:\n",
    "Because network definition are written as S-expressions, we can perform pattern matching to find internal components of an architecture. HyTorch contains a pattern-matching tool which can be used to return a list of matching sub-expressions found in the network definition. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; pmatch\n",
    "(defn pmatch? [expr pat]\n",
    "      (for [p ptuples] (print p)))\n",
    "\n",
    "; pmatch\n",
    "(defn pmatch [expr &rest ptuples]\n",
    "      (setv matches [])\n",
    "      (for [p ptuples] (print p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyExpression([\n",
      "  HySymbol('eval'),\n",
      "  HySymbol('print')])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pmatch `(eval 0) `(eval print))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"s44\"></a>\n",
    "### S-Expression Refactoring:\n",
    "Using pattermatching, we can quickly refactor components from our network arquitecture without having to rewrite any of the orginal code. HyTorch implements this using the following function which takes in pair of arguments defining the searched code and the desired refactored code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "; pmatch\n",
    "(defn refactor [expr &rest ptuples]\n",
    "      (setv matches [])\n",
    "      (for [p ptuples] (print p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"s5\"></a>\n",
    "## Network Analysis and Meta-Analysis Using HyTorch\n",
    "\n",
    "<a name=\"s51\"></a>\n",
    "### Fetching Internal Network Components:\n",
    "\n",
    "<a name=\"s52\"></a>\n",
    "### Probing Networks Using Tests:\n",
    "\n",
    "<a name=\"s53\"></a>\n",
    "### Loading Foreign Pytorch Models:\n",
    "\n",
    "<a name=\"s54\"></a>\n",
    "### Comparing Network Architectures:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Calysto Hy",
   "language": "hy",
   "name": "calysto_hy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "scheme"
   },
   "mimetype": "text/x-hylang",
   "name": "hy",
   "pygments_lexer": "lisp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
